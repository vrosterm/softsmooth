import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader, TensorDataset
from tqdm import tqdm

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
torch.manual_seed(0)

# Define the model architecture (DLA)
class BasicBlock(nn.Module):
    """Basic block for DLA with residual connections and three layers."""
    def __init__(self, in_channels, out_channels, stride=1):
        super(BasicBlock, self).__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.relu = nn.ReLU(inplace=True)
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_channels)
        self.conv3 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)  # New conv3
        self.bn3 = nn.BatchNorm2d(out_channels)  # New bn3
        self.shortcut = nn.Sequential()
        if stride != 1 or in_channels != out_channels:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(out_channels)
            )

    def forward(self, x):
        out = self.relu(self.bn1(self.conv1(x)))
        out = self.relu(self.bn2(self.conv2(out)))
        out = self.bn3(self.conv3(out))  # Apply the third layer
        out += self.shortcut(x)  # Residual connection
        return self.relu(out)
    
class DLA(nn.Module):
    """Deep Layer Aggregation (DLA) model."""
    def __init__(self, num_classes=10):
        super(DLA, self).__init__()
        self.base_layer = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=7, stride=1, padding=3, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True)
        )
        self.level0 = self._make_level(64, 64, 1, stride=1)
        self.level1 = self._make_level(64, 128, 2, stride=2)
        self.level2 = self._make_level(128, 256, 2, stride=2)
        self.level3 = self._make_level(256, 512, 2, stride=2)
        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(512, num_classes)

    def _make_level(self, in_channels, out_channels, num_blocks, stride):
        layers = [BasicBlock(in_channels, out_channels, stride)]
        for _ in range(1, num_blocks):
            layers.append(BasicBlock(out_channels, out_channels))
        return nn.Sequential(*layers)

    def forward(self, x):
        x = self.base_layer(x)
        x = self.level0(x)
        x = self.level1(x)
        x = self.level2(x)
        x = self.level3(x)
        x = self.global_pool(x)
        x = torch.flatten(x, 1)
        x = self.fc(x)
        return x

model_cnn = DLA(num_classes=10).to(device)

# Load the saved model state dictionary with weights_only=True
print("Loading model...")
model_cnn.load_state_dict(torch.load("model_CIFAR10.pt", weights_only=True))
model_cnn.eval()
print("Model loaded.")

# Load CIFAR10 test data
print("Loading CIFAR10 test data...")
CIFAR10_test = datasets.CIFAR10("C:/Users/walke/data", train=False, download=True, transform=transforms.ToTensor())
test_loader = DataLoader(CIFAR10_test, batch_size=100, shuffle=False)
print("CIFAR10 test data loaded.")

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
torch.manual_seed(0)

class Flatten(nn.Module):
    def forward(self, x):
        return x.view(x.shape[0], -1)

# PGD linf attack function
def pgd_linf(model, X, y, epsilon=0.03, alpha=0.003623, num_iter=1, randomize=False):
    if randomize:
        delta = torch.rand_like(X, requires_grad=True)
        delta.data = delta.data * 2 * epsilon - epsilon
    else:
        delta = torch.zeros_like(X, requires_grad=True)
        
    for t in range(num_iter):
        loss = nn.CrossEntropyLoss()(model(X + delta), y)
        loss.backward()
        delta.data = (delta + alpha*delta.grad.detach().sign()).clamp(-epsilon, epsilon)
        delta.grad.zero_()
    return delta.detach()

# Function to evaluate adversarial robustness
def epoch_adversarial(loader, model, attack, **kwargs):
    total_loss, total_err = 0., 0.
    robust_examples = []
    non_robust_examples = []
    print("Starting adversarial evaluation...")

    # Wrap the DataLoader with tqdm for a progress bar
    for X, y in tqdm(loader, desc="Adversarial Evaluation", leave=True):
        X, y = X.to(device), y.to(device)
        delta = attack(model, X, y, **kwargs)
        yp = model(X + delta)
        loss = nn.CrossEntropyLoss()(yp, y)
        total_err += (yp.max(dim=1)[1] != y).sum().item()
        total_loss += loss.item() * X.shape[0]
        
        # Partition data into robust and non-robust subsets
        correct = (yp.max(dim=1)[1] == y)
        robust_examples.append((X[correct].cpu(), y[correct].cpu()))
        non_robust_examples.append((X[~correct].cpu(), y[~correct].cpu()))
    
    robust_examples = [torch.cat(x) for x in zip(*robust_examples)]
    non_robust_examples = [torch.cat(x) for x in zip(*non_robust_examples)]
    
    print("Adversarial evaluation completed.")
    return total_err / len(loader.dataset), total_loss / len(loader.dataset), robust_examples, non_robust_examples

# Evaluate adversarial robustness
adv_err, adv_loss, robust_examples, non_robust_examples = epoch_adversarial(test_loader, model_cnn, pgd_linf)
print(f"Adversarial Accuracy: {100 - 100*adv_err:.2f}%")

# Save robust and non-robust data subsets
print("Saving robust and non-robust data subsets...")
robust_dataset = TensorDataset(*robust_examples)
non_robust_dataset = TensorDataset(*non_robust_examples)

torch.save(robust_dataset, "robust_dataset.pt")
torch.save(non_robust_dataset, "non_robust_dataset.pt")

print("Datasets saved.")